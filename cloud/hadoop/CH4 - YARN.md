# CHAPTER 4
- YARN 이란 하둡의 클러스터 자원 관리 시스템이다.
    - 사용자 코드 수준에서는 사용할 수 없고, 고수준 API를 통해 작성한다.

## 4.1 YARN 애플리케이션 수행 해부해보기

- YARN은 RM, NM 두가지 유형의 장기 실행 데몬으로 서비스 제공
    - RM: 클러스터 전체 자원의 사용량 관리
    - NM: 컨테이너를 구동하고 모니터링 하는 역할
- YARN 어플리케이션 구동 절차
    1. 클라이언트가 RM에 접속해 YARN 애플리케이션을 제출
    2. RM은 컨테이너에서 애플리케이션 마스터를 시작할 수 있는 NM을 하나 찾는다. 
    3. 애플리케이션 마스터는 딱 한번 실행되고 끝날수도 있고, RM에 더 많은 리소스 (컨테이너)를 요청하고 더 나아가서 분산 처리를 수행할 수도 있다. 

### 4.1.1 자원 요청

- 유연한 자원 요청 모델
    - 다수의 컨테이너 요청시 각 컨테이너에게 필요한 컴퓨팅 자원 뿐 아니라 ‘지역성 - locality' 제약도 표현이 가능하다. (네트워크 대역폭 때문에 중요하다)
    - 지역성 제약은 특정 노드나 랙, 또는 클러스터의 외부 랙에서 컨테이너를 요청할 때 사용된다.
- ex) 맵리듀스의 맵 태스크 실행 (HDFS에 블록에 접근할 컨테이너를 찾기)
    1. 복제본을 저장한 세개의 노드 중 하나에 컨테이너를 요청
    2. 안되면 랙의 다른 노드에 요청
    3. 안되면 클러스터 전체의 노드에서 찾는다. 
- 자원 요청의 경우
    - 처음 시작시 모든 요청을 하거나
        - ex) 스파크
    - 동적으로 자원을 추가로 요청하거나
        - ex) 리듀스 태스크는 맵 태스크 실행 후 실행될 수 있기 때문에 컨테이너를 추가로 요청할수도 있다.

### 4.1.2 애플리케이션의 수명

- 수명은 애플리케이션의 유형에 따라 매우 다를것이다.
    1. 사용자의 잡 당 하나의 애플리케이션 실행 (MR)
    2. 워크플로우나 사용자의 잡 세션당 하나의 애플리케이션 실행 (스파크)
        1. 순차적인 잡들이 동일한 컨테이너를 재사용할 수 있어 효율적
        2. 당연히 공유되는 데이터는 캐싱도 가능함
    3. 서로 다른 사용자들이 공유하는 애플리케이션

### 4.1.3 YARN 애플리케이션 만들기

- YARN 애플리케이션을 작성하기 용이하게 해주는 프로젝트들이 있다.
    - 아파치 슬라이더
        - 기존의 분산 애플리케이션을 YARN 위에서 실행하도록 한다.
    - 아파치 트윌
        - 분산 애플리케이션을 개발할 수 있는 간단한 프로그래밍 모델을 추가로 제공한다.

## 4.2 YARN과 맵리듀스 1의 차이점

- 하둡 구버전은 맵리듀스 1,
- YARN에서는 맵리듀스 2로 구분
- 맵리듀스 1
    - 잡의 실행과정을 제어하는 두개의 데몬
        - 잡트래커
            - 한개
            - 여러개의 태스크 트래커에서 실행되는 태스크를 스케쥴링
            - 잡 스케줄링 + 모니터링을 모두 담당
            - 다만 YARN은 이걸 분리해서 RM과 AM으로 처리한다.
        - 태스크트래커
            - 한개 이상
            - 태스크를 실행하고 상황을 잡트래커에게 전송
            - 태스크 실패하면 잡트래커는 다른 태스크 트래커에게 스케쥴링
            - YARN의 NM과 같은 역할
- YARN
    - 맵리듀스 1은 한계가 있고, YARN을 사용하면 다음 이득이 있다.
    - 확장성
        - 더 큰 클러스터에서 실행가능
        - 맵리듀스 1은 잡트래커가 관리하는 일을 모두 맡아서 하기 때문
        - AM은 해당 애플리케이션이 실행될때만 존재
    - 가용성 (HA - 고가용성)
        - 서비스 데몬에 문제 발생 시 필요한 작업들을 다른 데몬이 이어받아 실행할 수 있게 상태정보를 항상 복사해둔다
        - 원래 잡트래커 하나만 쓸때는 너무 어렵다. 상태가 수초마다 변경되기 때문
        - 하지만 이제 RM과 AM으로 분리되었다.
            - divide & conquer 문제로 바뀌었다.
            - 일단 RM에 HA를 제공
            - 그 후에 애플리케이션에 HA를 제공
    - 효율성
        - 원래는 고정된 크기 ‘슬롯'의 정적 할당 설정
            - 맵슬롯은 맵태스크에서만, 리듀스 슬롯은 리듀스 태스크에서만
        - YARN에서는 정해진 개수 말고 리소스 풀을 관리
            - 맵 슬롯에 사용할 리소스가 남으면 당연히 리듀스 태스크가 가져가서 쓸 수 있다.
    - 멀티 테넌시 (다중 사용자)
        - 다양한 애플리케이션을 실행할 수 있다. MR은 진짜 하나의 애플리케이션에 불과

## 4.3 YARN 스케줄링

- 스케줄링 정책을 사용자가 직접 선택할 수 있다.

### 4.3.1 스케줄러 옵션

- YARN이 제공하는 스케줄러
    - FIFO
        - 큐에 넣고 선입선출로 실행
        - 단순하긴한데 공유 클러스터 환경에서는 좋지 않다.
        - 대형 애플리케이션이 점유해버릴수 있기 때문
    - Capacity
        - 미리 잡을 위한 자원을 예약해둔다.
        - 작은 잡을 제출하는 즉시 분리된 전용 큐에서 처리
    - Fair
        - 모든 잡의 자원을 동적으로 분배한다.
        - 대형 잡이 먼저 시작하면 모든 자원을 주지만, 그 다음에 다른 잡이 제출되면 떼어서 걔한테 준다.
        - 떼어주는데 약간의 시간차가 있지만 아무튼 모두 공평하게 사용할 수 있다.
- 고급 옵션을 알아보자!

### 4.3.2 Capacity 스케줄러 설정

- 각 조직에게 클러스터의 가용량을 미리 할당한다.
    - 각자 분리된 전용큐를 가진다.
    - 이 큐를 또 분리할 수 있어서 사용자 그룹 사이에도 차별을 둘 수 있다.
    - 단일 큐 내부에 있는 애플리케이션들은 FIFO로 스케줄링
- 원래 단일 잡은 속해있는 큐의 가용량을 넘게 자원을 사용할 수 없다.
    - 다만 자원이 클러스터에 남아있으면 가져다 쓸 수 있다.
    - 이러면 큐의 가용량을 초과하게 되고, 이런걸 queue elasticity라고 한다.

### 4.3.3 페어 스케줄러 설정

- 모든 애플리케이션에게 동일하게 자원을 할당
    - 하지만 이 균등 배분은 큐 사이에만 실제로 적용된다.
    - 큐 내부로 들어가면 거기서 또 쪼개서 자원을 사용하게 된다.
- 예를 들어
    - 큐 A의 잡 a
    - 큐 B의 잡 b
    - 이렇게 시작하면 각 큐에 전체 자원의 50%씩 주지
    - 이때 큐 B에 잡 c가 들어오면 ..?
    - 잡 a, b, c가 각각 33%씩 가져가는게 아니라
    - a는 여전히 50%, b와 c가 각각 25%씩 가져간다.
- 각 큐에 서로다른 스케줄링 정책을 설정할수도 있다.
    - 기본은 페어스케줄링
    - 큐에는 FIFO도 쓸 수 있다.
- 큐 배치
    - 애플리케이션을 큐에 할당할 때 규칙 기반 시스템을 이용한다.
    - 맞는 규칙이 나올때까지 순서대로 시도
    - 큐를 명시적으로 지정할수도 있고, 아니면 기본적으로 사용자 이름의 큐를 사용 (없으면 생성)
- 선점 (Preemption)
    - 잡의 시작 시간을 어느정도 예측하기 위해서 필요
    - 자원의 균등 공유에 위배되는 큐에서 실행되는 컨테이너를 죽일 수 있다.
    - 하지만 중단되고 나면 반드시 다시 수행되어야한다.
    - 두가지 선점 타임아웃 설정이 있다.
        - 최소공유
            - 큐가 최소보장 자원을 받지 못하고 이게 지나면 다른 컨테이너를 선취할 수 있다.
        - 균등공유
            - 큐가 균등 공유의 절반 이하로 있는 시간이 지나면 선취할 수 있다.

### 4.3.4 지연 스케줄링

- YARN에서는 지역성 요청이 가장 우선시 된다.
    - 바쁜 클러스터에서는 특정 노드 요청시 이미 거기에 다른 컨테이너가 돌고 있을수 있다.
    - 이러면 지역성을 좀 낮춰서라도 동일 랙의 다른 노드에 컨테이너 할당할수도 있다.
    - 다만, 좀만 기다리면 요청한 노드에 할당받을 수 있는 기회가 확 올라간다.
        - 그래서 이걸 기다리는게 delay scheduling이다
- 모든 NM들은 주기적으로 (기본 1초) RM에 heartbeat를 보내 정보를 주고받는다.
- 지연 스케줄링 사용시 처음 온느 스케줄링 기회를 바로 사용하지 않는다.
    - 지역성을 낮추는걸 허용하는 횟수까지 기다리고 그 다음에 오는 기회를 잡는다.

### 4.3.5 우성 자원 공평성

- 메모리같이 분배할 자원이 하나의 유형이면 분배가 별로 어렵지 않다.
- 다만, CPU와 메모리를 동시에 생각해야한다면 ..?
    - A: 많은 CPU, 적은 메모리 요구
    - B: 적은 CPU, 많은 메모리 요구
- 이러면 두 사용자에서 우세한 자원을 측정기준으로 삼는다.
- 예를 들어 A에서는 CPU를, B에서는 메모리를 기준으로 삼아서 자원을 분배해주는것.
- ex) 클러스터에서 CPU 100개, 메모리 10TB
    - A: 2개의 CPU, 300GB 메모리 요구 → 각각 2%, 3% 따라서 3% 메모리가 우세
    - B: 6개의 CPU, 100GB 메모리 요구 → 각각 6%, 1% 따라서 6% CPU가 우세
    - B가 A보다 2배의 자원을 받게된다.
